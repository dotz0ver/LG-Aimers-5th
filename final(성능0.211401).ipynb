{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3zGbNKuf1YghQgYMF+P9k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Heus7DgEj-2","executionInfo":{"status":"ok","timestamp":1724682281893,"user_tz":-540,"elapsed":19766,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"9b466eec-284a-4721-fcf7-1f0fc7cf6d39"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install seaborn"],"metadata":{"id":"yQT_8M0yEnwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install imbalanced-learn"],"metadata":{"id":"0s2nwRbIEsin"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from pprint import pprint\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.impute import SimpleImputer\n","from imblearn.over_sampling import SMOTE\n","from tqdm import tqdm"],"metadata":{"id":"SjEBbyBCEqNS","executionInfo":{"status":"ok","timestamp":1724682284388,"user_tz":-540,"elapsed":2500,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"oMB8YRPuEGhA","executionInfo":{"status":"error","timestamp":1724787392929,"user_tz":-540,"elapsed":1264,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"dd296b02-9e36-4290-e392-de2016b32995"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2381d7286536>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["ROOT_DIR = \"/content/drive/MyDrive/LG_AIMERS/data\"\n","RANDOM_STATE = 110\n","\n","# Load data\n","train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n","test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n","\n","# Target distribution visualization\n","target_counts = train_data['target'].value_counts(normalize=True)\n","plt.figure(figsize=(6, 4))\n","target_counts.plot(kind='bar', color=['blue', 'orange'])\n","plt.title('Distribution of Normal and AbNormal in target')\n","plt.xlabel('Class')\n","plt.ylabel('Proportion')\n","plt.xticks(rotation=0)\n","plt.show()\n","\n","# Drop columns with common NaNs\n","nan_columns_train = train_data.columns[train_data.isna().any()].tolist()\n","nan_columns_test = test_data.columns[test_data.isna().any()].tolist()\n","common_nan_columns = list(set(nan_columns_train) & set(nan_columns_test))\n","train_data = train_data.drop(columns=common_nan_columns)\n","test_data = test_data.drop(columns=common_nan_columns)\n","\n","# Split features and target\n","train_x = train_data.drop(columns=[\"target\"])\n","train_y = train_data[\"target\"]\n","test_x = test_data.drop(columns=[\"Set ID\", \"target\"], errors='ignore')\n","test_set_id = test_data['Set ID']  # Save the Set ID column\n","\n","# Separate numeric and categorical columns\n","numeric_cols = train_x.select_dtypes(include=['number']).columns\n","categorical_cols = train_x.select_dtypes(include=['object']).columns\n","\n","# Fill missing values\n","numeric_imputer = SimpleImputer(strategy='median')\n","categorical_imputer = SimpleImputer(strategy='most_frequent')\n","train_x[numeric_cols] = numeric_imputer.fit_transform(train_x[numeric_cols])\n","test_x[numeric_cols] = numeric_imputer.transform(test_x[numeric_cols])\n","train_x[categorical_cols] = categorical_imputer.fit_transform(train_x[categorical_cols])\n","test_x[categorical_cols] = categorical_imputer.transform(test_x[categorical_cols])\n","\n","# Encode categorical variables\n","def preprocess_data(df, categorical_cols):\n","    label_encoders = {}\n","    for column in categorical_cols:\n","        le = LabelEncoder()\n","        df[column] = le.fit_transform(df[column])\n","        label_encoders[column] = le\n","    return df\n","\n","train_x = preprocess_data(train_x, categorical_cols)\n","test_x = preprocess_data(test_x, categorical_cols)\n","\n","# Additional Feature Engineering\n","\n","# WorkMode_High feature creation\n","train_x['WorkMode_High'] = train_x['WorkMode Collect Result_Fill2'] > 2.0\n","test_x['WorkMode_High'] = test_x['WorkMode Collect Result_Fill2'] > 2.0\n","\n","train_x['WorkMode_High'] = train_x['WorkMode_High'].astype(int)\n","test_x['WorkMode_High'] = test_x['WorkMode_High'].astype(int)\n","\n","# Create Model_Equipment_Combined feature\n","train_x['Model_Equipment_Combined'] = train_x['Model.Suffix_Dam'].astype(str) + \"_\" + train_x['Equipment_Dam'].astype(str)\n","test_x['Model_Equipment_Combined'] = test_x['Model.Suffix_Dam'].astype(str) + \"_\" + test_x['Equipment_Dam'].astype(str)\n","\n","# Binning WorkMode\n","train_x['WorkMode_Binned'] = pd.cut(train_x['WorkMode Collect Result_Fill2'], bins=[-np.inf, 1.0, 2.0, np.inf], labels=['Low', 'Medium', 'High'])\n","test_x['WorkMode_Binned'] = pd.cut(test_x['WorkMode Collect Result_Fill2'], bins=[-np.inf, 1.0, 2.0, np.inf], labels=['Low', 'Medium', 'High'])\n","\n","# Encode the new features\n","combined_factor_le = LabelEncoder()\n","train_x['Combined_Factor'] = (\n","    train_x['Equipment_Dam'].astype(str) + '_' +\n","    train_x['Model.Suffix_Dam'].astype(str) + '_' +\n","    train_x['WorkMode_High'].astype(str)\n",")\n","\n","test_x['Combined_Factor'] = (\n","    test_x['Equipment_Dam'].astype(str) + '_' +\n","    test_x['Model.Suffix_Dam'].astype(str) + '_' +\n","    test_x['WorkMode_High'].astype(str)\n",")\n","\n","train_x['Combined_Factor'] = combined_factor_le.fit_transform(train_x['Combined_Factor'])\n","test_x['Combined_Factor'] = combined_factor_le.transform(test_x['Combined_Factor'])\n","\n","# Use concat to add new columns to DataFrame at once\n","train_x = pd.concat([train_x, pd.get_dummies(train_x[['Combined_Factor', 'Model_Equipment_Combined', 'WorkMode_Binned']], drop_first=True)], axis=1)\n","test_x = pd.concat([test_x, pd.get_dummies(test_x[['Combined_Factor', 'Model_Equipment_Combined', 'WorkMode_Binned']], drop_first=True)], axis=1)\n","\n","# Drop original columns if necessary\n","train_x = train_x.drop(columns=['Combined_Factor', 'Model_Equipment_Combined', 'WorkMode_Binned'])\n","test_x = test_x.drop(columns=['Combined_Factor', 'Model_Equipment_Combined', 'WorkMode_Binned'])\n","\n","# Scaling features\n","scaler = StandardScaler()\n","train_x = pd.DataFrame(scaler.fit_transform(train_x), columns=train_x.columns)\n","test_x = pd.DataFrame(scaler.transform(test_x), columns=test_x.columns)\n","\n","# Balancing dataset using SMOTE\n","smote = SMOTE(random_state=RANDOM_STATE)\n","train_x, train_y = smote.fit_resample(train_x, train_y)\n","\n","# Split data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(\n","    train_x, train_y, test_size=0.3, stratify=train_y, random_state=RANDOM_STATE\n",")\n","\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, f1_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Random Forest, Gradient Boosting, KNN 모델 정의\n","rf_model = RandomForestClassifier(random_state=RANDOM_STATE)\n","gb_model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n","knn_model = KNeighborsClassifier()\n","\n","# Voting Classifier 정의 (소프트 보팅 방식)\n","voting_clf = VotingClassifier(estimators=[\n","    ('rf', rf_model),\n","    ('gb', gb_model),\n","    ('knn', knn_model)\n","], voting='soft')  # 'soft' 보팅을 사용해 모델의 확률을 기반으로 예측\n","\n","# 앙상블 모델 학습\n","voting_clf.fit(X_train, y_train)\n","\n","# 검증 데이터로 예측 수행\n","val_predictions = voting_clf.predict(X_val)\n","\n","# F1 Score 계산\n","f1 = f1_score(y_val, val_predictions, pos_label=\"AbNormal\")\n","print(f\"Voting Classifier F1 Score: {f1:.4f}\")\n","\n","# Classification Report 출력\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_val, val_predictions))\n","\n","# Confusion Matrix 시각화\n","cm = confusion_matrix(y_val, val_predictions, labels=[\"Normal\", \"AbNormal\"])\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"AbNormal\"], yticklabels=[\"Normal\", \"AbNormal\"])\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.title(\"Voting Classifier Confusion Matrix\")\n","plt.show()\n"]},{"cell_type":"code","source":["# 테스트 데이터 예측 및 결과 저장\n","test_predictions = voting_clf.predict(test_x)\n","\n","# 'Set ID' 열을 포함하여 결과를 저장\n","output = pd.DataFrame({'Set ID': test_set_id, 'Prediction': test_predictions})\n","\n","# 'target' 열 추가\n","output[\"target\"] = test_predictions\n","\n","# 제출 파일 저장 (Set ID가 포함된 상태로 저장)\n","output.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"AYcrbMtvEgcL"},"execution_count":null,"outputs":[]}]}